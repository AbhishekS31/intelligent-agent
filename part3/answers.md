# Part 3: Conceptual Understanding & System Proficiency

## 1. Agent Evaluation & Correctness

### a. How would you measure whether your agent is taking the correct action in response to the prompt?

To measure whether the agent is responding correctly, I would evaluate two key factors:

- **Tool Selection Accuracy**: Verify if the agent routes the prompt to the most relevant tool based on the defined system configuration. This is done using rule-based evaluation where prompts are mapped to expected tools, and actual tool usage is compared for correctness.

- **Response Validity**: Check that the output generated by the tool is coherent, contextually relevant, and successfully returned. This involves testing both structured content (e.g., valid weather data) and qualitative output (e.g., meaningful quotes or jokes).

A combined use of:
- Unit tests for routing validation
- An evaluation framework (`evaluate.py`)
- Manual or LLM-assisted reviews for subjective outputs  
helps ensure the correctness of agent behavior.

### b. Propose a mechanism to detect conflicting or stale results.

To detect conflicting or stale results:

- **For APIs (e.g., weather)**: Add a timestamp field to API responses and check:
  - If the data is recent (e.g., within the past hour)
  - If API response headers include caching metadata or version mismatch indicators

- **For static or cached tools (e.g., quotes, jokes)**:
  - Implement periodic rotation or updates to prevent stale repetition
  - Version responses or use content hashes to detect unexpected duplicates

Additionally, logging tool output and comparing it to recent history can help detect conflicts (e.g., same quote or joke being returned repeatedly) or outdated results.

---

## 2. Prompt Engineering Proficiency

### a. How do you design system prompts to guide agent behavior effectively?

System prompts are designed with the following goals:

- **Clear Intent Mapping**: Define a structured set of keywords or patterns that align user language to tool actions. These are used to route prompts accurately.
  
- **Tone and Personality**: Use a descriptive tone guide (e.g., "friendly and polite") to influence how fallback messages and tool outputs are framed.

- **Fallback Logic**: Include clear instructions for how to respond when no tool matches, or when a tool fails. This ensures graceful degradation of behavior.

- **Modular and Configurable**: The system prompt that I defined in `systemprompt_config.json` is easy to update, audit, and test.

### b. What constraints, tone, and structure do you enforce, and how do you test them?

**Constraints:**
- Only respond using defined tools or fallback messages
- Avoid hallucination or unsupported functionality
- Route based on keyword mapping (tool â†’ keywords list)

**Tone:**
- Responses should be concise, user-friendly, and polite
- Fallbacks should be polite and non-technical

**Structure:**
- Each response includes a result (`response`) and metadata (`tool_name`, `success`, `error_message`)
- Ensures every response is traceable and testable

**Testing:**
- Unit tests verify correct tool usage based on keywords
- Evaluation framework checks if fallback behavior activates correctly
- Can also use LLM-as-judge (as future extension) that can validate tone and coherence of output
